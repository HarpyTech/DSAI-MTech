{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are answers to Section A questions in detail:\n",
    "\n",
    "### Section A\n",
    "\n",
    "1. **a) Find \\( X \\) if \\(\\det\\begin{bmatrix} 2 & 4 \\\\ 5 & 1 + X \\end{bmatrix} = \\det\\begin{bmatrix} 2X & 4 \\\\ 6 & X + 2 \\end{bmatrix}\\):**\n",
    "\n",
    "   To solve for \\( X \\), equate the determinants of the two matrices.\n",
    "\n",
    "   \\[\n",
    "   \\text{Determinant of } \\begin{bmatrix} 2 & 4 \\\\ 5 & 1 + X \\end{bmatrix} = 2(1 + X) - 4 \\cdot 5 = 2X + 2 - 20 = 2X - 18\n",
    "   \\]\n",
    "\n",
    "   For the second matrix:\n",
    "   \\[\n",
    "   \\text{Determinant of } \\begin{bmatrix} 2X & 4 \\\\ 6 & X + 2 \\end{bmatrix} = 2X(X + 2) - 4 \\cdot 6 = 2X^2 + 4X - 24\n",
    "   \\]\n",
    "\n",
    "   Setting the determinants equal:\n",
    "   \\[\n",
    "   2X - 18 = 2X^2 + 4X - 24\n",
    "   \\]\n",
    "   Simplify and solve for \\( X \\) by rearranging and factoring or using the quadratic formula.\n",
    "\n",
    "---\n",
    "\n",
    "1. **b) Calculate the angle between the vectors \\( a = i + 2j \\) and \\( b = 9i + 3j \\):**\n",
    "\n",
    "   The angle \\( \\theta \\) between two vectors \\( a \\) and \\( b \\) can be calculated using:\n",
    "   \\[\n",
    "   \\cos \\theta = \\frac{a \\cdot b}{|a| |b|}\n",
    "   \\]\n",
    "   where:\n",
    "   \\[\n",
    "   a \\cdot b = (1)(9) + (2)(3) = 9 + 6 = 15\n",
    "   \\]\n",
    "   The magnitudes are:\n",
    "   \\[\n",
    "   |a| = \\sqrt{1^2 + 2^2} = \\sqrt{5}, \\quad |b| = \\sqrt{9^2 + 3^2} = \\sqrt{81 + 9} = \\sqrt{90} = 3\\sqrt{10}\n",
    "   \\]\n",
    "   So,\n",
    "   \\[\n",
    "   \\cos \\theta = \\frac{15}{\\sqrt{5} \\cdot 3\\sqrt{10}} = \\frac{15}{3 \\cdot \\sqrt{50}} = \\frac{15}{3 \\cdot 5 \\sqrt{2}} = \\frac{1}{\\sqrt{2}}\n",
    "   \\]\n",
    "   Therefore, \\( \\theta = \\cos^{-1}\\left(\\frac{1}{\\sqrt{2}}\\right) = 45^\\circ \\).\n",
    "\n",
    "---\n",
    "\n",
    "1. **c) Determine whether the function \\( f(x) = -8x^2 + 15 \\) is concave or convex:**\n",
    "\n",
    "   A function is concave if its second derivative is negative and convex if its second derivative is positive.\n",
    "\n",
    "   \\[\n",
    "   f(x) = -8x^2 + 15\n",
    "   \\]\n",
    "   First derivative: \\( f'(x) = -16x \\)\n",
    "\n",
    "   Second derivative: \\( f''(x) = -16 \\)\n",
    "\n",
    "   Since \\( f''(x) = -16 < 0 \\), the function \\( f(x) = -8x^2 + 15 \\) is **concave**.\n",
    "\n",
    "---\n",
    "\n",
    "1. **d) Find the vector projection of \\( a = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix} \\) on \\( b = \\begin{bmatrix} 8 \\\\ 2 \\end{bmatrix} \\):**\n",
    "\n",
    "   The vector projection of \\( a \\) onto \\( b \\) is given by:\n",
    "   \\[\n",
    "   \\text{Proj}_{b} a = \\frac{a \\cdot b}{b \\cdot b} b\n",
    "   \\]\n",
    "   First, calculate \\( a \\cdot b \\) and \\( b \\cdot b \\):\n",
    "   \\[\n",
    "   a \\cdot b = 5 \\cdot 8 + 5 \\cdot 2 = 40 + 10 = 50\n",
    "   \\]\n",
    "   \\[\n",
    "   b \\cdot b = 8^2 + 2^2 = 64 + 4 = 68\n",
    "   \\]\n",
    "   So,\n",
    "   \\[\n",
    "   \\text{Proj}_{b} a = \\frac{50}{68} \\begin{bmatrix} 8 \\\\ 2 \\end{bmatrix} = \\frac{25}{34} \\begin{bmatrix} 8 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{200}{34} \\\\ \\frac{50}{34} \\end{bmatrix} = \\begin{bmatrix} \\frac{100}{17} \\\\ \\frac{25}{17} \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "1. **e) What is the effect of a higher learning rate in the Gradient Descent algorithm?**\n",
    "\n",
    "   A higher learning rate can make the algorithm converge faster; however, it may also cause the algorithm to overshoot the minimum, leading to divergence or oscillation. A very high learning rate prevents the gradient descent from settling at the minimum, as it may skip over it in each iteration.\n",
    "\n",
    "---\n",
    "\n",
    "2. **a) Write the transformation matrix for the rotation of a 2D image:**\n",
    "\n",
    "   A rotation transformation matrix for a 2D image by an angle \\( \\theta \\) is:\n",
    "   \\[\n",
    "   R(\\theta) = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "2. **b) What will happen when eigenvalues are roughly equal?**\n",
    "   \n",
    "   - **II. PCA will perform badly**\n",
    "   \n",
    "   When eigenvalues are nearly equal, it indicates that there is no clear direction of maximum variance, which makes it difficult for PCA to identify significant patterns or directions in the data.\n",
    "\n",
    "---\n",
    "\n",
    "2. **c) Calculate the Jacobian matrix for the functions \\( f_1(x,y) = x^3 y \\) and \\( f_2(x,y) = x^2 + y^2 \\):**\n",
    "\n",
    "   The Jacobian matrix \\( J \\) for functions \\( f_1 \\) and \\( f_2 \\) with respect to variables \\( x \\) and \\( y \\) is:\n",
    "   \\[\n",
    "   J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{bmatrix}\n",
    "   \\]\n",
    "   Calculate each partial derivative:\n",
    "   \\[\n",
    "   \\frac{\\partial f_1}{\\partial x} = 3x^2 y, \\quad \\frac{\\partial f_1}{\\partial y} = x^3\n",
    "   \\]\n",
    "   \\[\n",
    "   \\frac{\\partial f_2}{\\partial x} = 2x, \\quad \\frac{\\partial f_2}{\\partial y} = 2y\n",
    "   \\]\n",
    "   So,\n",
    "   \\[\n",
    "   J = \\begin{bmatrix} 3x^2 y & x^3 \\\\ 2x & 2y \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "2. **d) In simple linear regression for a single data point \\( (x_i, y_i) \\), find \\( \\frac{\\partial L}{\\partial w_0} \\) and \\( \\frac{\\partial L}{\\partial w_1} \\):**\n",
    "\n",
    "   The loss function for a single data point is:\n",
    "   \\[\n",
    "   L(w_0, w_1) = (y_i - (w_0 + w_1 x_i))^2\n",
    "   \\]\n",
    "   Taking partial derivatives:\n",
    "   \\[\n",
    "   \\frac{\\partial L}{\\partial w_0} = -2(y_i - (w_0 + w_1 x_i))\n",
    "   \\]\n",
    "   \\[\n",
    "   \\frac{\\partial L}{\\partial w_1} = -2x_i(y_i - (w_0 + w_1 x_i))\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "2. **e) RGB image transformation:**\n",
    "\n",
    "   The new image is created by concatenating slices from an RGB image. Each slice corresponds to different channels, so the new image will likely display unusual color distortions as the channel combinations differ from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are Python code solutions for each question in Section B. \n",
    "\n",
    "---\n",
    "\n",
    "### 3a. Assess the Relationship Between Nifty and Stock A Data (2015-2019)\n",
    "\n",
    "We can calculate the correlation coefficient between Nifty and Stock A prices using Python to determine if there’s a relationship.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    \"Year\": [2015, 2016, 2017, 2018, 2019],\n",
    "    \"Nifty\": [1692, 1978, 1884, 2151, 2519],\n",
    "    \"Stock_A\": [68, 102, 110, 112, 154]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation = df[\"Nifty\"].corr(df[\"Stock_A\"])\n",
    "print(\"Correlation coefficient between Nifty and Stock A:\", correlation)\n",
    "```\n",
    "\n",
    "If the correlation coefficient is close to 1 or -1, it indicates a strong linear relationship. A value close to 0 implies a weak relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### 3b. Rate of Increase in Area of Circular Wave\n",
    "\n",
    "Given \\( r = 8 \\) cm and \\( \\frac{dr}{dt} = 5 \\) cm/s, we can calculate the rate of change of the area \\( \\frac{dA}{dt} \\) using Python.\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# Given values\n",
    "r = 8  # cm\n",
    "dr_dt = 5  # cm/s\n",
    "\n",
    "# Area rate of change\n",
    "dA_dt = 2 * math.pi * r * dr_dt\n",
    "print(\"Rate of change of area:\", dA_dt, \"cm^2/s\")\n",
    "```\n",
    "\n",
    "This will yield the rate at which the area is increasing.\n",
    "\n",
    "---\n",
    "\n",
    "### 3c. Convolution of an Image with a Specific Kernel\n",
    "\n",
    "For image convolution, suppose we have a middle pixel intensity of 99 and a kernel matrix. Here’s how we can apply convolution to a small example.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "# Define a sample 3x3 image patch and convolution kernel\n",
    "image_patch = np.array([[80, 90, 85], [70, 99, 65], [60, 75, 95]])\n",
    "kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
    "\n",
    "# Perform convolution\n",
    "result = convolve(image_patch, kernel, mode='constant', cval=0.0)\n",
    "print(\"Convolved image patch:\\n\", result)\n",
    "print(\"New intensity of middle pixel:\", result[1, 1])\n",
    "```\n",
    "\n",
    "This will calculate the new intensity for the middle pixel and show the effect of the convolution on the entire patch.\n",
    "\n",
    "---\n",
    "\n",
    "### 3d. Transformation of Coordinates\n",
    "\n",
    "Using a transformation matrix \\( T \\), we can calculate the transformed coordinates of an object. For this example, assume a rotation by 45 degrees as the transformation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define original coordinates and rotation matrix for 45 degrees\n",
    "coordinates = np.array([[1, 0], [0, 1], [-1, 0], [0, -1]])\n",
    "theta = np.radians(45)\n",
    "T = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "# Apply transformation\n",
    "transformed_coordinates = coordinates @ T.T\n",
    "print(\"Transformed coordinates:\\n\", transformed_coordinates)\n",
    "```\n",
    "\n",
    "This code outputs the coordinates after applying a 45-degree rotation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3e. Covariance Matrix for Height and Weight Data\n",
    "\n",
    "Using the weight and height data provided, we can compute the covariance matrix.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Data: weights in pounds, heights in inches\n",
    "weights = np.array([120, 125, 125, 135, 145])\n",
    "heights = np.array([61, 60, 64, 68, 72])\n",
    "\n",
    "# Stack the data as columns\n",
    "data = np.stack((weights, heights), axis=0)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = np.cov(data)\n",
    "print(\"Covariance matrix:\\n\", cov_matrix)\n",
    "```\n",
    "\n",
    "The output will be a 2x2 covariance matrix showing the covariance between weights and heights.\n",
    "\n",
    "---\n",
    "\n",
    "### 3f. Eigenvalues of Matrix \\( A \\) and \\( A^2 \\)\n",
    "\n",
    "For matrix \\( A = \\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix} \\), we can find the eigenvalues of \\( A \\) and \\( A^2 \\).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define matrix A\n",
    "A = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Eigenvalues of A\n",
    "eigenvalues_A, _ = np.linalg.eig(A)\n",
    "print(\"Eigenvalues of A:\", eigenvalues_A)\n",
    "\n",
    "# Eigenvalues of A^2 (can be obtained by squaring eigenvalues of A)\n",
    "eigenvalues_A2 = eigenvalues_A ** 2\n",
    "print(\"Eigenvalues of A^2:\", eigenvalues_A2)\n",
    "```\n",
    "\n",
    "This code computes the eigenvalues of \\( A \\) and then squares them to obtain the eigenvalues of \\( A^2 \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When eigenvalues are roughly equal in PCA, it implies that the variances along the corresponding eigenvector directions are similar. Here’s what this might mean and some follow-up questions that can deepen understanding around PCA behavior and outcomes:\n",
    "\n",
    "### 1. What happens when eigenvalues are roughly equal?\n",
    "   - **Interpretation**: When eigenvalues are similar, it indicates that there isn't a dominant direction of variance in the data. This can make it challenging to decide which components to keep for dimensionality reduction since no direction is significantly more informative than the others.\n",
    "   - **Impact**: The lack of distinct eigenvalues may imply that the data is spread out relatively evenly across multiple dimensions. This can lead to a less effective reduction since dropping components may result in a more noticeable loss of information.\n",
    "\n",
    "### Additional Questions on PCA and Eigenvalues/Eigenvectors\n",
    "\n",
    "2. **What if the largest eigenvalue is very small?**\n",
    "   - If the largest eigenvalue is small, it indicates that the overall variance in the data is low. This could mean that the data points are tightly clustered, and PCA might not be as effective for dimensionality reduction because there is limited variance to capture.\n",
    "\n",
    "3. **How does PCA handle data with correlated features?**\n",
    "   - When features are highly correlated, PCA tends to combine these features into a few components with large eigenvalues. This results in dimensionality reduction as the components capture shared variance, reducing redundancy in the data.\n",
    "\n",
    "4. **What if some eigenvalues are zero or close to zero?**\n",
    "   - Zero or near-zero eigenvalues suggest that certain dimensions in the data have negligible variance, meaning they contribute very little information. PCA will naturally eliminate these dimensions, which often correspond to redundant or linearly dependent features.\n",
    "\n",
    "5. **How does the choice of scaling (standardization) affect eigenvalues?**\n",
    "   - If features are not standardized, features with larger scales will dominate the variance structure, leading to higher eigenvalues along those dimensions. Standardizing data to have unit variance equalizes feature contributions and can lead to a more balanced distribution of eigenvalues, allowing PCA to focus on the intrinsic variance structure.\n",
    "\n",
    "6. **How does the dimensionality of data affect the number of non-zero eigenvalues?**\n",
    "   - The number of non-zero eigenvalues corresponds to the rank of the data matrix. If data has a lower rank than its dimensionality (e.g., due to redundancy), PCA will yield fewer non-zero eigenvalues, implying that fewer principal components are required to represent the data accurately.\n",
    "\n",
    "7. **What happens to PCA if there are outliers in the data?**\n",
    "   - Outliers can disproportionately affect eigenvalues and eigenvectors, as they introduce large variance along certain directions. This can lead to components that capture outlier behavior rather than intrinsic patterns, potentially skewing PCA results.\n",
    "\n",
    "8. **How does the interpretability of principal components change as dimensionality is reduced?**\n",
    "   - As dimensionality is reduced, the retained principal components represent aggregated patterns rather than individual features. This can make them harder to interpret directly, as each component is a combination of original features.\n",
    "\n",
    "9. **What if data does not follow a Gaussian distribution?**\n",
    "   - PCA assumes that most of the variance is meaningful, which holds best when data is Gaussian. Non-Gaussian data may have patterns that aren't captured purely by variance, and alternative techniques like ICA (Independent Component Analysis) may be more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
