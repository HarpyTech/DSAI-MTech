{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nvjoO5CWI03T",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCEaJDV9JV59"
   },
   "source": [
    "Section B: Text Preprocessing and Word Embedding (40 Marks)\n",
    "\n",
    "Question 2: Text Preprocessing (20 Marks)\n",
    "\n",
    "Given the dataset reviews.csv, perform the following preprocessing steps:\n",
    "\n",
    "(a) Load the dataset and display the first 5 rows. (5 Marks)\n",
    "\n",
    "(b) Remove any duplicate reviews. (5 Marks)\n",
    "\n",
    "(c) Clean the text by removing punctuation, converting to lowercase, and removing stopwords. (10 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Load the dataset and display the first 5 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UqnmzG-uKhgD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product exceeded my expectations! It's hi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product was decent. It worked fine, but it...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had a terrible experience with this company....</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's an okay product. Nothing to write home ab...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disappointed with the product. It didn't meet ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiment\n",
       "0  This product exceeded my expectations! It's hi...  Positive\n",
       "1  The product was decent. It worked fine, but it...   Neutral\n",
       "2  I had a terrible experience with this company....  Negative\n",
       "3  It's an okay product. Nothing to write home ab...   Neutral\n",
       "4  Disappointed with the product. It didn't meet ...  Negative"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Remove any duplicate reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Clean the text by removing punctuation, converting to lowercase, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to lowercase\n",
    "df['Review'] = df['Review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "import string\n",
    "\n",
    "df['Review'] = df['Review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\PRIYA\n",
      "[nltk_data]     DAS/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idyNoW1hKKWA"
   },
   "source": [
    "Question 3: Word Embedding (20 Marks)\n",
    "\n",
    "(a) Convert the cleaned text into word embeddings using TF-IDF(10 Marks)\n",
    "\n",
    "(b) Display the shape of the resulting TF-IDF matrix. (4 Marks)\n",
    "\n",
    "(c) Get the vocabulary of TF-IDF  (6 marks)\n",
    "\n",
    "\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Convert the cleaned text into word embeddings using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 1), stop_words= 'english')\n",
    "\n",
    "df_tfidf_vectors = tfidf_vectorizer.fit_transform(df['Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Display the shape of the resulting TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 147)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Get the vocabulary of TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product', 'exceeded', 'expectations', 'highquality', 'performs', 'exceptionally', 'decent', 'worked', 'fine', 'wasnt', 'special', 'terrible', 'experience', 'company', 'customer', 'service', 'rude', 'unhelpful', 'okay', 'write', 'home', 'disappointed', 'didnt', 'meet', 'avoid', 'costs', 'products', 'subpar', 'outstanding', 'exactly', 'looking', 'works', 'perfectly', 'absolutely', 'horrendous', 'topquality', 'satisfied', 'purchase', 'im', 'extremely', 'dissatisfied', 'cheaply', 'doesnt', 'function', 'properly', 'satisfactory', 'recommend', 'overpriced', 'work', 'advertised', 'exceptional', 'representatives', 'friendly', 'helpful', 'great', 'quality', 'average', 'particularly', 'good', 'bad', 'poor', 'worth', 'price', 'highly', 'blow', 'away', 'recommended', 'fantastic', 'knowledgeable', 'excellent', 'mediocre', 'job', 'awful', 'expected', 'item', 'money', 'worst', 'ive', 'received', 'faulty', 'unresponsive', 'neutral', 'stand', 'better', 'impressive', 'horrible', 'atrocious', 'meh', 'regret', 'buying', 'poorly', 'impressed', 'definitely', 'return', 'incredible', 'prompt', 'needed', 'happier', 'purchasing', 'love', 'met', 'remarkable', 'amazing', 'buy', 'dreadful', 'staff', 'shop', 'stay', 'brilliant', 'went', 'arrived', 'promptly', 'wellpackaged', 'pleasantly', 'surprised', 'attention', 'delivery', 'late', 'damaged', 'loved', 'superb', 'like', 'plague', 'disappointing', 'unprofessional', 'craftsmanship', 'overall', 'time', 'described', 'noteworthy', 'topnotch', 'encountered', 'extraordinary', 'delighted', 'help', 'issue', 'amazed', 'durable', 'complete', 'waste', 'come', 'bought', 'disappointment', 'resolved', 'quickly', 'packaging', 'unremarkable']\n"
     ]
    }
   ],
   "source": [
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "print(list(vocab.keys()))  # this prints all words in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQiRLo4wKXm7"
   },
   "source": [
    "Section B: Model Building for Sentiment Analysis (40 Marks)\n",
    "\n",
    "Question 4: Naive Bayes Classifier (20 Marks)\n",
    "\n",
    "(a) Split the dataset into training and testing sets. (5 Marks)\n",
    "\n",
    "(b) Train a Naive Bayes classifier on the training set. (10 Marks)\n",
    "\n",
    "(c) Evaluate the model on the testing set and display the accuracy. (5 Marks)\n",
    "\n",
    "Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Split the dataset into training and testing sets. (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(df_tfidf_vectors, y, test_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_vectors, y, test_size= 0.2, random_state= 42, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Train a Naive Bayes classifier on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_model = nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Evaluate the model on the testing set and display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predict = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629629629629629"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_accuracy = accuracy_score(y_test, nb_predict)\n",
    "nb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sPRuBr_KqBB"
   },
   "source": [
    "Question 5: LSTM Model (20 Marks)\n",
    "\n",
    "(a) Convert the cleaned text into sequences using Tokenizer. (5 Marks)\n",
    "\n",
    "(b) Build and compile an LSTM model for sentiment analysis. (10 Marks)\n",
    "\n",
    "(c) Train the model and evaluate its performance on the testing set. (5 Marks)\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Convert the cleaned text into sequences using Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. Encode Sentiment to 0 and 1\n",
    "le = LabelEncoder()\n",
    "df['Sentiment'] = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# 2. Tokenize Review text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['Review'])\n",
    "\n",
    "# 3. Convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['Review'])\n",
    "\n",
    "# 4. Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Build and compile an LSTM model for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=64, input_length=100))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Train the model and evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 7s 611ms/step - loss: 0.6625 - accuracy: 0.3173 - val_loss: 0.6020 - val_accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 0.5692 - accuracy: 0.3173 - val_loss: 0.4804 - val_accuracy: 0.3333\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.4257 - accuracy: 0.3173 - val_loss: 0.2107 - val_accuracy: 0.3333\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.1155 - accuracy: 0.3173 - val_loss: 0.0323 - val_accuracy: 0.3333\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.0034 - accuracy: 0.3173 - val_loss: 0.0118 - val_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0118 - accuracy: 0.3333\n",
      "LSTM Test Accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split padded sequences and encoded labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences, df['Sentiment'], test_size=0.2, stratify=df['Sentiment'], random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"LSTM Test Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tfenv38)",
   "language": "python",
   "name": "tfenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
