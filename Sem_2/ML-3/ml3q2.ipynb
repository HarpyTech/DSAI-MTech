{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "nE422OVPNO_n",
    "outputId": "b75a0410-1725-4b97-b970-5b2c9557e525"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-747954a4-d684-4443-a610-9a6644501743\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-747954a4-d684-4443-a610-9a6644501743\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving clust_data.csv to clust_data.csv\n",
      "Saving recommendation_mini.csv to recommendation_mini.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aEdcPZWZOsNZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-MV_h-9CvEJ",
    "outputId": "3d152ff2-ca93-4438-e2f2-ea7a4d32114c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Data:\n",
      "          F1         F2          F3         F4          F5          F6  \\\n",
      "0  -3.026950  78.111415 -374.114333  70.234478   47.055428 -132.530866   \n",
      "1  10.666065  38.688616    0.982882 -90.163689 -209.167839  -15.156554   \n",
      "2  12.889724   6.537806   -7.172982  13.354590 -104.688780   14.148796   \n",
      "3 -13.518929  93.875045  -93.384710 -20.934313  -21.779267   21.189977   \n",
      "4  -1.892682  -0.630952  -61.208627 -34.865905  -12.535872  -12.747138   \n",
      "\n",
      "           F7          F8          F9         F10  ...         F12        F13  \\\n",
      "0 -650.397369  209.966341    9.216498  152.958095  ... -116.980754  21.669078   \n",
      "1  -64.164883  157.497306 -147.046024  443.664845  ... -214.024783 -25.270747   \n",
      "2  -47.564563  -83.617770 -127.093760 -155.737080  ...   69.016811 -17.877942   \n",
      "3   97.753321  -71.051340   89.128396  -80.578483  ...   49.643950 -11.838531   \n",
      "4  -95.405479  -13.823230  -48.152023   95.988291  ...   89.092312   2.923746   \n",
      "\n",
      "         F14         F15         F16        F17        F18         F19  \\\n",
      "0  31.152334  -70.343341   48.442535   7.248523 -56.082621  209.013176   \n",
      "1 -37.412225  -40.705016  137.410740  10.278976  55.812041  176.724972   \n",
      "2  -3.705984   73.796777   80.636683   9.667143  95.658938  -40.108126   \n",
      "3   6.372738  -12.364559   82.427811   5.176107   3.056736  474.834028   \n",
      "4   3.147485 -108.133807   62.718187  -4.398725 -43.235521  207.724802   \n",
      "\n",
      "          F20  target  \n",
      "0 -134.510804       2  \n",
      "1  -99.276276       3  \n",
      "2 -242.796912       2  \n",
      "3  -18.943667       1  \n",
      "4  208.727105       1  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Cluster Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   F1      10000 non-null  float64\n",
      " 1   F2      10000 non-null  float64\n",
      " 2   F3      10000 non-null  float64\n",
      " 3   F4      10000 non-null  float64\n",
      " 4   F5      10000 non-null  float64\n",
      " 5   F6      10000 non-null  float64\n",
      " 6   F7      10000 non-null  float64\n",
      " 7   F8      10000 non-null  float64\n",
      " 8   F9      10000 non-null  float64\n",
      " 9   F10     10000 non-null  float64\n",
      " 10  F11     10000 non-null  float64\n",
      " 11  F12     10000 non-null  float64\n",
      " 12  F13     10000 non-null  float64\n",
      " 13  F14     10000 non-null  float64\n",
      " 14  F15     10000 non-null  float64\n",
      " 15  F16     10000 non-null  float64\n",
      " 16  F17     10000 non-null  float64\n",
      " 17  F18     10000 non-null  float64\n",
      " 18  F19     10000 non-null  float64\n",
      " 19  F20     10000 non-null  float64\n",
      " 20  target  10000 non-null  int64  \n",
      "dtypes: float64(20), int64(1)\n",
      "memory usage: 1.6 MB\n",
      "\n",
      "Recommendation Data:\n",
      "   UserID  ItemID  Rating  Timestamp\n",
      "0     905     470       1  889325071\n",
      "1     697    1518       5  879835275\n",
      "2     855    1687       5  875638677\n",
      "3     950    1447       5  877420720\n",
      "4     806    1170       4  879889337\n",
      "\n",
      "Recommendation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85724 entries, 0 to 85723\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   UserID     85724 non-null  int64\n",
      " 1   ItemID     85724 non-null  int64\n",
      " 2   Rating     85724 non-null  int64\n",
      " 3   Timestamp  85724 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "# prompt: Dataset Information: cluster_data.csv\n",
    "#  The experiments have been carried out with a group of 30 volunteers within an age bracket\n",
    "#  of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS,\n",
    "#  WALKING_DOWNSTAIRS,SITTING, STANDING, LAYING) wearing a smartphone\n",
    "#  (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, they\n",
    "#  have captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of\n",
    "#  50Hz. The experiments have been video-recorded to label the data manually.\n",
    "#  This Dataset consist of\n",
    "#  A561-feature vector with time and frequency domain variables.\n",
    "#  Its activity label.\n",
    "#  An identifier of the subject who carried out the experiment\n",
    "#  Dataset Information: recommendation.csv\n",
    "#  The recommendation.csv file consists of 85724 ratings given by 943 users on 1659\n",
    "#  products.\n",
    "#  akshaypurushan@gmail.com\n",
    "#  CQAR0OSH88\n",
    "#  It has the following 4 columns:\n",
    "#  UserID\n",
    "#  ItemID\n",
    "#  Rating (Integers 1 to 5)\n",
    "#  Timestamp (Unix time stamp).\n",
    "#  Note:\n",
    "#  1.Use cluster_data.csv for all the clustering and dimensionality reduction questions\n",
    "#  2.Use recommendations.csv for recommendation system questions\n",
    "#  3.'activity' column in the cluster_data.csv is the target column. Don't use this column for\n",
    "#  clustering purposes. You can use this for predictive model building\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "cluster_data = pd.read_csv('clust_data.csv')\n",
    "recommendation_data = pd.read_csv('recommendation_mini.csv')\n",
    "\n",
    "# Display the first few rows and information about each dataset\n",
    "print(\"Cluster Data:\")\n",
    "print(cluster_data.head())\n",
    "print(\"\\nCluster Data Info:\")\n",
    "cluster_data.info()\n",
    "\n",
    "print(\"\\nRecommendation Data:\")\n",
    "print(recommendation_data.head())\n",
    "print(\"\\nRecommendation Data Info:\")\n",
    "recommendation_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wHZ0GDMJPLAd"
   },
   "outputs": [],
   "source": [
    "# prompt:  Perform required pre-processing and compute how many pairs of variables have the\n",
    "#  correlation more than 0.8 ? Apply PCA and compute the required number of principal\n",
    "#  components to capture the 90 percent variance of the original data. Print the Eigenvalues and\n",
    "#  Eigenvectors of top 5 PCs\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pre-processing the recommendation_data for correlation and PCA\n",
    "# Assuming numerical columns are relevant for correlation and PCA\n",
    "recommendation_numeric = recommendation_data.select_dtypes(include=np.number)\n",
    "\n",
    "# Drop columns with missing values or handle them appropriately\n",
    "# For simplicity, dropping columns with any missing values for demonstration\n",
    "recommendation_numeric_cleaned = recommendation_numeric.dropna(axis=1)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = recommendation_numeric_cleaned.corr()\n",
    "\n",
    "# Find pairs with correlation > 0.8 (excluding self-correlation)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
    "\n",
    "print(f\"\\nNumber of pairs with correlation > 0.8: {len(high_corr_pairs)}\")\n",
    "# print(\"High correlation pairs:\", high_corr_pairs) # Uncomment to see the pairs\n",
    "\n",
    "# Scale the data before applying PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(recommendation_numeric_cleaned)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Compute the required number of components to capture 90% variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components_90 = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "\n",
    "print(f\"\\nNumber of principal components to capture 90% variance: {n_components_90}\")\n",
    "\n",
    "# Print Eigenvalues (explained variance) of top 5 PCs\n",
    "print(\"\\nEigenvalues (Explained Variance) of top 5 PCs:\")\n",
    "for i in range(min(5, len(pca.explained_variance_))):\n",
    "    print(f\"PC {i+1}: {pca.explained_variance_[i]:.4f}\")\n",
    "\n",
    "# Print Eigenvectors (components) of top 5 PCs\n",
    "print(\"\\nEigenvectors of top 5 PCs:\")\n",
    "# Each row of components_ corresponds to a principal component\n",
    "# Each column of components_ corresponds to an original feature\n",
    "for i in range(min(5, len(pca.components_))):\n",
    "    print(f\"PC {i+1}: {pca.components_[i]}\")\n",
    "\n",
    "# Optional: Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance by Number of Components')\n",
    "plt.axhline(y=0.90, color='r', linestyle='-', label='90% Variance')\n",
    "plt.axvline(x=n_components_90, color='g', linestyle='-', label=f'{n_components_90} Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIddQ-iBUoxw"
   },
   "outputs": [],
   "source": [
    "# prompt: Build the K-means clustering model with reduced PCA features (PCs which are explaining 90\n",
    "#  percent variance) and compute the optimal value of clusters. Make the business inferences\n",
    "#  using the cluster groups.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply PCA with the number of components explaining 90% variance\n",
    "pca_final = PCA(n_components=n_components_90)\n",
    "pca_data = pca_final.fit_transform(scaled_data)\n",
    "\n",
    "print(f\"\\nShape of PCA reduced data: {pca_data.shape}\")\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method and Silhouette Score\n",
    "# Using Elbow Method\n",
    "inertia = []\n",
    "for k in range(1, 11): # Test k from 1 to 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(pca_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Using Silhouette Score (for k > 1)\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11): # Silhouette score requires at least 2 clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(pca_data)\n",
    "    score = silhouette_score(pca_data, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal k')\n",
    "plt.xticks(range(2, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Based on the Elbow and Silhouette plots, choose an optimal k\n",
    "# Let's assume from the plots, k=3 or k=4 appears optimal for demonstration\n",
    "optimal_k = 3 # Replace with the value you determine from the plots\n",
    "\n",
    "print(f\"\\nChoosing optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Build K-means model with the optimal k\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans_optimal.fit_predict(pca_data)\n",
    "\n",
    "# Add the cluster labels back to the original cleaned recommendation data\n",
    "recommendation_numeric_cleaned['Cluster'] = clusters\n",
    "\n",
    "# Analyze the characteristics of each cluster\n",
    "print(\"\\nCluster Analysis:\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = recommendation_numeric_cleaned[recommendation_numeric_cleaned['Cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} (Number of data points: {len(cluster_data)}):\")\n",
    "    print(cluster_data.drop('Cluster', axis=1).describe().T) # Describe the numerical features in the cluster\n",
    "\n",
    "# Business Inferences based on cluster analysis\n",
    "# The interpretation here depends heavily on the actual features in your 'recommendation_numeric_cleaned' data.\n",
    "# Replace the comments below with actual interpretations based on your data's columns and the .describe() output.\n",
    "\n",
    "print(\"\\nBusiness Inferences:\")\n",
    "# Example: If your data contains purchase frequency, average order value, etc.\n",
    "# Based on the .describe() output for each cluster, you can infer:\n",
    "\n",
    "# Cluster 0: (Example) Might represent high-value customers - high purchase frequency, high average order value.\n",
    "# Potential Business Actions: Offer loyalty programs, personalized high-end product recommendations, dedicated support.\n",
    "\n",
    "# Cluster 1: (Example) Might represent new or infrequent customers - low purchase frequency, lower average order value.\n",
    "# Potential Business Actions: Targeted promotions to encourage repeat purchases, onboarding campaigns, product education.\n",
    "\n",
    "# Cluster 2: (Example) Might represent bargain hunters - moderate purchase frequency, low average order value, focused on discounted items.\n",
    "# Potential Business Actions: Send notifications about sales and discounts, bundle deals, promote value-for-money products.\n",
    "\n",
    "# You would need to look at the means/medians/standard deviations of the original scaled or unscaled features within each cluster\n",
    "# (by describing the cluster_data) to make specific inferences relevant to your business and data.\n",
    "# For a more robust analysis, you might also consider analyzing categorical features if they were included initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTFOvbKsUvjs"
   },
   "outputs": [],
   "source": [
    "# prompt: Build/Plot the top 100 cluster dendogram using 4 different linkages and compare its\n",
    "#  performance.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Use the PCA reduced data for hierarchical clustering\n",
    "# We will work with a sample of the data for plotting the dendrogram\n",
    "# as plotting for 100 samples is more manageable for visualization.\n",
    "# Let's sample 100 data points from the PCA data.\n",
    "np.random.seed(42) # for reproducibility\n",
    "sample_size = min(100, pca_data.shape[0])\n",
    "sample_indices = np.random.choice(pca_data.shape[0], size=sample_size, replace=False)\n",
    "pca_data_sample = pca_data[sample_indices]\n",
    "\n",
    "# Calculate the distance matrix\n",
    "distance_matrix = pdist(pca_data_sample)\n",
    "\n",
    "# Define different linkage methods\n",
    "linkage_methods = ['ward', 'average', 'complete', 'single']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    # Perform hierarchical clustering using the current linkage method\n",
    "    linked = linkage(distance_matrix, method)\n",
    "\n",
    "    # Plot the dendrogram\n",
    "    dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "    plt.title(f'Dendrogram (Linkage: {method}) - Top {sample_size} Samples')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance Comparison (Qualitative based on Dendrogram Visualization)\n",
    "print(\"\\nPerformance Comparison of Linkage Methods (Qualitative based on Dendrogram):\")\n",
    "print(\"- Ward: Tends to produce compact, spherical clusters. Minimizes the variance of the clusters being merged.\")\n",
    "print(\"- Average: Uses the average distance between all pairs of observations between the two sets of data. Can produce more balanced trees than single linkage.\")\n",
    "print(\"- Complete: Uses the maximum distance between all pairs of observations between the two sets of data. Tends to produce more compact clusters, but sensitive to outliers.\")\n",
    "print(\"- Single: Uses the minimum distance between all pairs of observations between the two sets of data. Tends to produce long, 'straggly' clusters (chaining effect), sensitive to noise.\")\n",
    "print(\"\\nChoosing the best linkage method depends on the structure of the data and the desired cluster properties.\")\n",
    "print(\"Visual inspection of the dendrograms helps in understanding how clusters are formed at different distance thresholds.\")\n",
    "print(\"For quantitative comparison, one might use metrics like cophenetic correlation coefficient (though not explicitly shown in the plot itself).\")\n",
    "print(\"However, the request was to compare performance based on plotting the dendrogram, which is primarily a visual assessment of how clearly separated and meaningful the clusters appear at different linkage levels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlXYTx0kU1fc"
   },
   "outputs": [],
   "source": [
    "# prompt:  Clusterthedatainto5groupsusingK-meansandordertheclustersintermsofthe\n",
    "#  inertia(WCSS)ofeachcluster.\n",
    "\n",
    "import numpy as np\n",
    "# Clusterthedatainto5groupsusingK-means\n",
    "n_clusters_kmeans = 5\n",
    "kmeans_5 = KMeans(n_clusters=n_clusters_kmeans, random_state=42, n_init=10)\n",
    "kmeans_5.fit(pca_data)\n",
    "clusters_5 = kmeans_5.labels_\n",
    "\n",
    "# Get the inertia (WCSS) for each cluster\n",
    "# We can access the inertia for the overall clustering, but to get WCSS for each cluster,\n",
    "# we need to calculate it manually based on the assigned points and cluster centroids.\n",
    "# The inertia_ attribute of the KMeans model gives the sum of squared distances of samples\n",
    "# to their closest cluster center for *all* clusters.\n",
    "\n",
    "# Calculate WCSS for each cluster manually\n",
    "cluster_inertia = []\n",
    "for i in range(n_clusters_kmeans):\n",
    "    # Get data points belonging to this cluster\n",
    "    cluster_points = pca_data[clusters_5 == i]\n",
    "    # Get the centroid for this cluster\n",
    "    cluster_centroid = kmeans_5.cluster_centers_[i]\n",
    "    # Calculate the sum of squared distances from points to the centroid\n",
    "    inertia_i = np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "    cluster_inertia.append((i, inertia_i))\n",
    "\n",
    "# Order the clusters by inertia (WCSS) in ascending order\n",
    "ordered_clusters = sorted(cluster_inertia, key=lambda item: item[1])\n",
    "\n",
    "print(f\"\\nWCSS (Inertia) for each cluster (ordered by WCSS):\")\n",
    "for cluster_id, inertia_value in ordered_clusters:\n",
    "    print(f\"Cluster {cluster_id}: {inertia_value:.2f}\")\n",
    "\n",
    "# Add the 5-cluster labels to the dataframe (optional, but often useful)\n",
    "# recommendation_numeric_cleaned['Cluster_5'] = clusters_5\n",
    "# print(\"\\nRecommendation Data with 5-Cluster Labels:\")\n",
    "# print(recommendation_numeric_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwUO-W5rU8ku"
   },
   "outputs": [],
   "source": [
    "# prompt: BuildthefollowingMLmodelandcompareitsperformance:\n",
    "#  a.MLmodelwithoriginalinp_dataandout\n",
    "#  b.MLmodelwithpca_inp_dataandoutput\n",
    "#  c.MLmodelwithsvd_inp_dataandout\n",
    "#  d.MLmodelwithlda_inp_dataandout\n",
    "#  Note1:The‘activity’columninthedatasetistheoutputcolumn(out)\n",
    "#  Note2:\n",
    "#  inp_data▯Allthecolumnsintheoriginaldataset(excluding‘activity’)\n",
    "#  pca_inp_data▯number of PCAcomponentswhich captures the 95 percent of\n",
    "#  variancesvd_inp_data▯numberofSVDcomponentswhichcapturesthe95percent\n",
    "#  ofvariancelda_inp_data▯requirednumberofLDAcomponents\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import time\n",
    "\n",
    "# Assuming recommendation_data is already loaded and cleaned as recommendation_numeric_cleaned\n",
    "\n",
    "# Drop non-numeric columns if any were introduced later (like 'Cluster')\n",
    "# Re-select only the original numeric columns for consistency before splitting\n",
    "# Make sure 'activity' column is present in the original 'recommendation_data' and is the output\n",
    "# If 'activity' is not in recommendation_data but is in cluster_data, we need to adjust.\n",
    "# Let's assume 'activity' is the target variable and is in the original recommendation_data.\n",
    "# We will need to merge or ensure 'activity' is available.\n",
    "\n",
    "# Re-load the recommendation data to ensure we have the 'activity' column\n",
    "# assuming 'activity' is in the original 'recommendation_mini.csv'\n",
    "try:\n",
    "    recommendation_data_full = pd.read_csv('recommendation_mini.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'recommendation_mini.csv' not found. Please ensure it is uploaded or in the correct path.\")\n",
    "    # Exit or handle the error appropriately\n",
    "    exit()\n",
    "\n",
    "# Separate the target variable ('activity')\n",
    "if 'activity' in recommendation_data_full.columns:\n",
    "    out = recommendation_data_full['activity']\n",
    "    inp_data_full = recommendation_data_full.drop('activity', axis=1)\n",
    "else:\n",
    "    print(\"Error: 'activity' column not found in 'recommendation_mini.csv'. Cannot proceed.\")\n",
    "    # Exit or handle the error appropriately\n",
    "    exit()\n",
    "\n",
    "# Select only numeric columns from the input features\n",
    "inp_data_numeric = inp_data_full.select_dtypes(include=np.number)\n",
    "\n",
    "# Handle missing values in the input data if any (e.g., imputation or dropping)\n",
    "# For simplicity, we'll drop columns with missing values again, consistent with previous steps.\n",
    "inp_data = inp_data_numeric.dropna(axis=1)\n",
    "\n",
    "# Ensure the target variable 'out' corresponds to the rows in 'inp_data' after dropping NaNs\n",
    "# We need to align 'out' with the rows that remained in 'inp_data'.\n",
    "# Since we dropped columns, the rows should still align. If we dropped rows, we'd need to filter 'out'.\n",
    "# Assuming dropping columns doesn't change the row order/indices, this alignment is fine.\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inp_data, out, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data (important for PCA, SVD, LDA, and many models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- a. ML model with original inp_data and out ---\n",
    "print(\"\\n--- Training ML Model with Original Data ---\")\n",
    "start_time = time.time()\n",
    "model_original = LogisticRegression(max_iter=1000) # Increased max_iter for potential convergence issues\n",
    "model_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = model_original.predict(X_test_scaled)\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "report_original = classification_report(y_test, y_pred_original)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Accuracy: {accuracy_original:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_original)\n",
    "\n",
    "\n",
    "# --- Determine number of components for PCA and SVD (95% variance) ---\n",
    "# Fit PCA on the scaled training data to find components for 95% variance\n",
    "pca_for_components = PCA()\n",
    "pca_for_components.fit(X_train_scaled)\n",
    "cumulative_variance = np.cumsum(pca_for_components.explained_variance_ratio_)\n",
    "# number of PCA components which captures the 95 percent of variance\n",
    "n_components_pca = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
    "print(f\"\\nNumber of PCA components for 95% variance: {n_components_pca}\")\n",
    "\n",
    "# Fit SVD on the scaled training data to find components for 95% variance\n",
    "svd_for_components = TruncatedSVD(n_components=X_train_scaled.shape[1] - 1) # Max components is n_features - 1 or min(n_samples, n_features)\n",
    "svd_for_components.fit(X_train_scaled)\n",
    "cumulative_variance_svd = np.cumsum(svd_for_components.explained_variance_ratio_)\n",
    "# number of SVD components which capture the 95 percent of variance\n",
    "# Need to handle cases where 95% variance is not reached even with max components\n",
    "n_components_svd = X_train_scaled.shape[1] - 1\n",
    "if np.any(cumulative_variance_svd >= 0.95):\n",
    "     n_components_svd = np.where(cumulative_variance_svd >= 0.95)[0][0] + 1\n",
    "else:\n",
    "    print(\"Warning: 95% variance not captured by SVD with max components. Using max possible components.\")\n",
    "\n",
    "print(f\"Number of SVD components for 95% variance: {n_components_svd}\")\n",
    "\n",
    "# --- Determine number of components for LDA ---\n",
    "# LDA's number of components is min(n_classes - 1, n_features)\n",
    "n_classes = len(np.unique(y_train))\n",
    "# required number of LDA components\n",
    "n_components_lda = min(n_classes - 1, X_train_scaled.shape[1])\n",
    "print(f\"Required number of LDA components: {n_components_lda}\")\n",
    "\n",
    "# --- b. ML model with pca_inp_data and output ---\n",
    "print(\"\\n--- Training ML Model with PCA Reduced Data ---\")\n",
    "# Apply PCA with the determined number of components\n",
    "pca_final = PCA(n_components=n_components_pca)\n",
    "X_train_pca = pca_final.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca_final.transform(X_test_scaled)\n",
    "\n",
    "start_time = time.time()\n",
    "model_pca = LogisticRegression(max_iter=1000)\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "report_pca = classification_report(y_test, y_pred_pca)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Accuracy: {accuracy_pca:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_pca)\n",
    "\n",
    "\n",
    "# --- c. ML model with svd_inp_data and out ---\n",
    "print(\"\\n--- Training ML Model with SVD Reduced Data ---\")\n",
    "# Apply SVD with the determined number of components\n",
    "svd_final = TruncatedSVD(n_components=n_components_svd, random_state=42)\n",
    "X_train_svd = svd_final.fit_transform(X_train_scaled)\n",
    "X_test_svd = svd_final.transform(X_test_scaled)\n",
    "\n",
    "start_time = time.time()\n",
    "model_svd = LogisticRegression(max_iter=1000)\n",
    "model_svd.fit(X_train_svd, y_train)\n",
    "y_pred_svd = model_svd.predict(X_test_svd)\n",
    "accuracy_svd = accuracy_score(y_test, y_pred_svd)\n",
    "report_svd = classification_report(y_test, y_pred_svd)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Accuracy: {accuracy_svd:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_svd)\n",
    "\n",
    "\n",
    "# --- d. ML model with lda_inp_data and out ---\n",
    "print(\"\\n--- Training ML Model with LDA Reduced Data ---\")\n",
    "# LDA requires the target variable for fitting\n",
    "# Check if LDA is applicable (n_components_lda > 0 and n_samples >= n_classes)\n",
    "if n_components_lda > 0 and X_train_scaled.shape[0] >= n_classes:\n",
    "    try:\n",
    "        # Apply LDA with the determined number of components\n",
    "        lda_final = LinearDiscriminantAnalysis(n_components=n_components_lda)\n",
    "        X_train_lda = lda_final.fit_transform(X_train_scaled, y_train)\n",
    "        X_test_lda = lda_final.transform(X_test_scaled)\n",
    "\n",
    "        start_time = time.time()\n",
    "        model_lda = LogisticRegression(max_iter=1000)\n",
    "        model_lda.fit(X_train_lda, y_train)\n",
    "        y_pred_lda = model_lda.predict(X_test_lda)\n",
    "        accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "        report_lda = classification_report(y_test, y_pred_lda)\n",
    "        end_time = time.time()\n",
    "        print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "        print(f\"Accuracy: {accuracy_lda:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report_lda)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not perform LDA: {e}\")\n",
    "        print(\"LDA might not be applicable due to data characteristics or insufficient samples per class.\")\n",
    "else:\n",
    "    print(\"\\n--- Skipping LDA ---\")\n",
    "    print(f\"LDA is not applicable. Required LDA components: {n_components_lda}. Number of training samples: {X_train_scaled.shape[0]}. Number of classes: {n_classes}.\")\n",
    "\n",
    "\n",
    "# --- Comparison of Performance ---\n",
    "print(\"\\n--- Performance Comparison ---\")\n",
    "print(f\"Model with Original Data Accuracy: {accuracy_original:.4f}\")\n",
    "if 'accuracy_pca' in locals():\n",
    "    print(f\"Model with PCA Data Accuracy: {accuracy_pca:.4f}\")\n",
    "if 'accuracy_svd' in locals():\n",
    "    print(f\"Model with SVD Data Accuracy: {accuracy_svd:.4f}\")\n",
    "if 'accuracy_lda' in locals():\n",
    "     print(f\"Model with LDA Data Accuracy: {accuracy_lda:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Compare the Accuracy scores for each model.\")\n",
    "print(\"- Compare the Classification Reports (Precision, Recall, F1-score) for each class.\")\n",
    "print(\"- Consider the training time for each model (especially relevant for larger datasets).\")\n",
    "print(\"- Dimensionality reduction techniques (PCA, SVD, LDA) can sometimes improve model performance (e.g., by reducing noise, multicollinearity) or reduce training time, but can also lead to information loss and decreased performance.\")\n",
    "print(\"- The best approach depends on the specific dataset and the characteristics of the features and target variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG0st37wVF6L"
   },
   "outputs": [],
   "source": [
    "# prompt: Usethedataset:recommendation.csv\n",
    "#  Buildthepopularitybasedrecommendationsystemandsuggesttop5items.\n",
    "\n",
    "# Popularity-based recommendation system\n",
    "# This approach recommends items that are most popular across all users.\n",
    "# popularity is typically measured by the number of times an item appears\n",
    "# in the dataset (e.g., number of ratings, purchases, etc.).\n",
    "\n",
    "# Assuming 'recommendation_data' contains a column that represents items\n",
    "# and implicit interactions (like presence implies interaction/interest).\n",
    "# A simple approach is to count the occurrences of each item.\n",
    "\n",
    "# Let's assume your recommendation_mini.csv has columns like 'user_id', 'item_id', etc.\n",
    "# We need a column that identifies the items. Let's assume 'item_id' exists.\n",
    "# If 'item_id' doesn't exist, you need to use a relevant item identifier column from your dataset.\n",
    "\n",
    "# Check if 'item_id' column exists\n",
    "if 'item_id' in recommendation_data.columns:\n",
    "    # Calculate the popularity of each item by counting occurrences\n",
    "    item_popularity = recommendation_data['item_id'].value_counts().reset_index()\n",
    "    item_popularity.columns = ['item_id', 'popularity_count']\n",
    "\n",
    "    # Sort items by popularity in descending order\n",
    "    popular_items = item_popularity.sort_values(by='popularity_count', ascending=False)\n",
    "\n",
    "    # Suggest the top 5 most popular items\n",
    "    top_n = 5\n",
    "    top_5_popular_items = popular_items.head(top_n)\n",
    "\n",
    "    print(f\"\\n--- Top {top_n} Popular Items ---\")\n",
    "    print(top_5_popular_items)\n",
    "\n",
    "else:\n",
    "    print(\"\\nError: 'item_id' column not found in the recommendation data.\")\n",
    "    print(\"Please ensure your 'recommendation_mini.csv' dataset has a column identifying items.\")\n",
    "    print(\"If your item identifier has a different column name, please update the code accordingly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkhPuYU7VMHB"
   },
   "outputs": [],
   "source": [
    "# prompt: Usethedataset:recommendation.csv\n",
    "#  Buildacollaborativerecommendationenginetorecommendthetop5itemstothespecific\n",
    "#  user.MeasurethemodelqualityintermsofRMSE\n",
    "\n",
    "!pip install scikit-surprise\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming recommendation_data_full is already loaded and contains 'user_id', 'item_id', and a rating/interaction column.\n",
    "# If your dataset structure is different, adjust the column names below.\n",
    "# Let's assume the interaction is represented by a rating or a value indicating interaction strength.\n",
    "# If you only have user_id and item_id (implicit feedback), you might need to create a dummy rating.\n",
    "# For this example, let's assume 'activity' is the rating/interaction column. If not, replace 'activity' with the correct column name.\n",
    "\n",
    "# Define a Reader object. The rating_scale parameter is important.\n",
    "# If 'activity' is a binary indicator (0 or 1), rating_scale should be (0, 1).\n",
    "# If it's a rating scale (e.g., 1-5), set it accordingly.\n",
    "# If it's implicit feedback, you might use a dummy rating of 1 for all interactions.\n",
    "# Let's assume 'activity' is a measure of interaction strength, treat it as a rating.\n",
    "# We need to check the range of the 'activity' column to set the rating_scale.\n",
    "# Let's assume it's a positive value indicating interaction strength.\n",
    "\n",
    "# Check the data types and range of 'activity' column\n",
    "if 'activity' in recommendation_data_full.columns:\n",
    "    activity_min = recommendation_data_full['activity'].min()\n",
    "    activity_max = recommendation_data_full['activity'].max()\n",
    "    print(f\"\\n'activity' column range: [{activity_min}, {activity_max}]\")\n",
    "    reader = Reader(rating_scale=(activity_min, activity_max))\n",
    "\n",
    "    # Load the data from the pandas DataFrame\n",
    "    # The order of columns in the DataFrame should be: user_id, item_id, rating\n",
    "    # Ensure these columns exist in your recommendation_data_full dataframe.\n",
    "    # If your user ID or item ID columns have different names, change 'user_id' and 'item_id'.\n",
    "    try:\n",
    "        data = Dataset.load_from_df(recommendation_data_full[['user_id', 'item_id', 'activity']], reader)\n",
    "    except KeyError as e:\n",
    "        print(f\"\\nError: Required column not found for collaborative filtering: {e}\")\n",
    "        print(\"Please ensure 'user_id', 'item_id', and 'activity' columns exist in 'recommendation_data_full'.\")\n",
    "        # Exit or handle the error appropriately\n",
    "        exit()\n",
    "\n",
    "    # Split data into training and testing sets for evaluating the model\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Use the SVD algorithm for collaborative filtering\n",
    "    # SVD is a matrix factorization technique commonly used in recommendation systems.\n",
    "    algo = SVD(random_state=42)\n",
    "\n",
    "    # Train the algorithm on the training set\n",
    "    print(\"\\nTraining SVD model...\")\n",
    "    algo.fit(trainset)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    print(\"Making predictions on test set...\")\n",
    "    predictions = algo.test(testset)\n",
    "    print(\"Predictions complete.\")\n",
    "\n",
    "    # Measure the model quality in terms of RMSE\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "    print(f\"\\nRMSE on the test set: {rmse:.4f}\")\n",
    "\n",
    "    # --- Recommendation for a specific user ---\n",
    "    # Define the user ID you want recommendations for\n",
    "    # Replace 'specific_user_id' with an actual user ID from your dataset\n",
    "    # For demonstration, let's pick the first user ID from the training set\n",
    "    specific_user_id = trainset.ir[0] # Get the internal user id of the first user in the training set\n",
    "    # To get the original user ID, you can use trainset.to_raw_uid(specific_user_id)\n",
    "    raw_specific_user_id = trainset.to_raw_uid(specific_user_id)\n",
    "\n",
    "    print(f\"\\nGenerating recommendations for user: {raw_specific_user_id}\")\n",
    "\n",
    "    # Get a list of all unique items\n",
    "    all_items = list(recommendation_data_full['item_id'].unique())\n",
    "\n",
    "    # Get items the user has already interacted with in the training set\n",
    "    # Convert raw user ID to internal user ID if needed, but predict expects raw IDs\n",
    "    try:\n",
    "        user_interacted_items = recommendation_data_full[recommendation_data_full['user_id'] == raw_specific_user_id]['item_id'].tolist()\n",
    "    except KeyError:\n",
    "         print(f\"Warning: User ID {raw_specific_user_id} not found in the original recommendation_data_full for checking interacted items.\")\n",
    "         user_interacted_items = [] # Assume no prior interactions found if user ID is not in the original df\n",
    "\n",
    "    # Find items the user has NOT interacted with\n",
    "    items_to_predict = [item for item in all_items if item not in user_interacted_items]\n",
    "\n",
    "    # Predict ratings for items the user has not interacted with\n",
    "    # Create a list of (user_id, item_id, dummy_rating) tuples for prediction\n",
    "    # Dummy rating is not used for prediction calculation itself, but required by predict function\n",
    "    predictions_for_user = [(raw_specific_user_id, item, 0) for item in items_to_predict]\n",
    "\n",
    "    # Get predicted ratings\n",
    "    predicted_ratings = algo.test(predictions_for_user)\n",
    "\n",
    "    # Sort predictions by estimated rating in descending order\n",
    "    top_n = 5\n",
    "    top_recommendations = sorted(predicted_ratings, key=lambda x: x.est, reverse=True)[:top_n]\n",
    "\n",
    "    print(f\"\\nTop {top_n} recommendations for user {raw_specific_user_id}:\")\n",
    "    for recommendation in top_recommendations:\n",
    "        # recommendation is a Prediction object: (uid, iid, r_ui, est, details)\n",
    "        print(f\"Item ID: {recommendation.iid}, Estimated Rating: {recommendation.est:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nError: 'activity' column not found in the recommendation data.\")\n",
    "    print(\"Please ensure your 'recommendation_mini.csv' dataset has a column representing the interaction strength (rating).\")\n",
    "    print(\"If your interaction column has a different name, please update the code accordingly.\")\n",
    "    print(\"If your data only has user and item IDs (implicit feedback), you may need to adjust the approach.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
