{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the Model\n",
    "\n",
    "* **Biased Error**: Error produced by the model during the Fitting Stage (Training Stage).\n",
    "* **Variance Error**: Difference in prediction when model fits into diffrent data set.\n",
    "\n",
    "1. Biased Error is Low, Varinace Error is High then the model is ***Over Fitted Model***\n",
    "2. Biased Error is High, Varinace Error is Low then the model is ***Under Fitted Model***\n",
    "\n",
    "How to handle overfitted model ?\n",
    "> **Regularization**: \n",
    "> * Rich\n",
    "> * Lasso\n",
    "> * Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE = $\\sum((Y_a - Y_p)^2)$\n",
    "\n",
    "= $\\sum((y-b_1 \\times x_1 - b_0)^2)$\n",
    "\n",
    "=  $\\sum((y-b_1 \\times x_1 - b_0)^2) + P$ here the $P$ is penality constant is called ***Regularization*** \n",
    "\n",
    "= $\\sum((y -b_1 \\times x_1 - b_2 \\times x_2 - b_0)^2) + \\lambda (\\beta1 ^ 2 + \\beta2 ^ 2)$ here the lamda of beta is called Hyper Parameter $L_2$.\n",
    "\n",
    "= $\\sum((y -b_1 \\times x_1 - b_2 \\times x_2 - b_0)^2) + \\lambda (\\beta1 + \\beta2)$ here the lamda of beta is called Hyper Parameter $L_1$ norm which defines ***Lasso Regularization***.\n",
    "\n",
    "= $\\sum((y -b_1 \\times x_1 - b_2 \\times x_2 - b_0)^2) + L_2 + L_1$ which defines ***Elasto Regularization***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) What is Machine Learning? State any two types of machine learning.\n",
    "\n",
    "**Machine Learning** is a branch of artificial intelligence that focuses on building systems that can learn from and make decisions based on data. It involves algorithms that improve their performance as they are exposed to more data over time.\n",
    "\n",
    "**Two types of machine learning**:\n",
    "1. **Supervised Learning**: The algorithm is trained on labeled data, meaning the input comes with the correct output. The goal is to learn a mapping from inputs to outputs. Examples include classification and regression.\n",
    "2. **Unsupervised Learning**: The algorithm is trained on unlabeled data and must find patterns and relationships within the data. Examples include clustering and association.\n",
    "\n",
    "### b) How can you handle overfitting and underfitting?\n",
    "\n",
    "**Overfitting** occurs when a model learns the training data too well, including noise and outliers, leading to poor generalization to new data. **Underfitting** occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "**Handling Overfitting**:\n",
    "1. **Cross-Validation**: Use techniques like k-fold cross-validation to ensure the model generalizes well to unseen data.\n",
    "2. **Regularization**: Apply regularization techniques like L1 (Lasso) or L2 (Ridge) to penalize large coefficients and reduce model complexity.\n",
    "\n",
    "**Handling Underfitting**:\n",
    "1. **Increase Model Complexity**: Use a more complex model that can capture the underlying patterns in the data.\n",
    "2. **Feature Engineering**: Add more relevant features or transform existing features to provide the model with more information.\n",
    "\n",
    "### c) State the assumptions of the linear regression algorithm.\n",
    "\n",
    "The assumptions of linear regression are:\n",
    "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
    "2. **Independence**: The residuals (errors) are independent.\n",
    "3. **Homoscedasticity**: The residuals have constant variance at every level of the independent variable.\n",
    "4. **Normality**: The residuals of the model are normally distributed.\n",
    "\n",
    "### d) If \\( y = 2x_1 + 12x_2 + 3x_3 + 5 \\) is the linear regression equation, then explain how the coefficients of \\( x_1 \\) and \\( x_2 \\) affect the value of \\( y \\).\n",
    "\n",
    "In the linear regression equation \\( y = 2x_1 + 12x_2 + 3x_3 + 5 \\):\n",
    "- The coefficient of \\( x_1 \\) is 2, which means that for every one-unit increase in \\( x_1 \\), \\( y \\) increases by 2 units, assuming all other variables remain constant.\n",
    "- The coefficient of \\( x_2 \\) is 12, which means that for every one-unit increase in \\( x_2 \\), \\( y \\) increases by 12 units, assuming all other variables remain constant.\n",
    "\n",
    "### e) Explain any two of the data preprocessing steps.\n",
    "\n",
    "1. **Normalization/Standardization**: This step involves scaling the data to a standard range, typically between 0 and 1 (normalization) or to have a mean of 0 and a standard deviation of 1 (standardization). This helps in improving the performance and convergence speed of the learning algorithms.\n",
    "2. **Handling Missing Values**: This step involves dealing with missing data points in the dataset. Techniques include removing rows with missing values, imputing missing values with mean, median, or mode, or using more advanced methods like K-Nearest Neighbors imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's go through each of these questions one by one.\n",
    "\n",
    "### 1a) State a few applications of Machine Learning\n",
    "\n",
    "Machine Learning has a wide range of applications, including:\n",
    "1. **Image and Speech Recognition**: Used in facial recognition systems, voice assistants like Siri and Alexa.\n",
    "2. **Healthcare**: Predicting disease outbreaks, personalized treatment plans, and medical image analysis.\n",
    "3. **Finance**: Fraud detection, algorithmic trading, and credit scoring.\n",
    "4. **Marketing**: Customer segmentation, recommendation systems, and sentiment analysis.\n",
    "5. **Autonomous Vehicles**: Self-driving cars use machine learning for object detection and decision-making.\n",
    "\n",
    "### 2b) What is the difference between Classification and Regression problem?\n",
    "\n",
    "- **Classification**: Involves predicting a categorical label. For example, determining whether an email is spam or not spam.\n",
    "- **Regression**: Involves predicting a continuous value. For example, predicting the price of a house based on its features.\n",
    "\n",
    "### 2c) Mention any two assumptions of linear regression algorithm.\n",
    "\n",
    "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
    "2. **Homoscedasticity**: The residuals have constant variance at every level of the independent variable.\n",
    "\n",
    "### 2d) How can you deal with multicollinearity?\n",
    "\n",
    "Multicollinearity can be dealt with by:\n",
    "1. **Removing Highly Correlated Predictors**: Identify and remove one of the highly correlated variables.\n",
    "2. **Principal Component Analysis (PCA)**: Transform the correlated variables into a set of linearly uncorrelated variables.\n",
    "\n",
    "### 2e) Explain some issues with Machine Learning.\n",
    "\n",
    "Some issues with Machine Learning include:\n",
    "1. **Overfitting**: The model performs well on training data but poorly on new, unseen data.\n",
    "2. **Underfitting**: The model is too simple to capture the underlying patterns in the data.\n",
    "3. **Data Quality**: Poor quality data can lead to inaccurate models.\n",
    "4. **Bias and Fairness**: Models can inherit biases present in the training data, leading to unfair outcomes.\n",
    "\n",
    "### 2a) Explain Forward Selection in brief.\n",
    "\n",
    "**Forward Selection** is a feature selection technique where we start with no features and iteratively add the most significant feature at each step. The process continues until adding new features does not significantly improve the model's performance.\n",
    "\n",
    "### 2b) If \\( y = 2x_1 + 12x_2 + 3x_3 + 5 \\) is the linear regression equation, then explain how the coefficients of \\( x_2 \\) and \\( x_3 \\) affect the value of \\( y \\).\n",
    "\n",
    "In the linear regression equation \\( y = 2x_1 + 12x_2 + 3x_3 + 5 \\):\n",
    "- The coefficient of \\( x_2 \\) is 12, which means that for every one-unit increase in \\( x_2 \\), \\( y \\) increases by 12 units, assuming all other variables remain constant.\n",
    "- The coefficient of \\( x_3 \\) is 3, which means that for every one-unit increase in \\( x_3 \\), \\( y \\) increases by 3 units, assuming all other variables remain constant.\n",
    "\n",
    "### 2c) How can you handle overfitting and underfitting?\n",
    "\n",
    "**Overfitting** can be handled by:\n",
    "1. **Cross-Validation**: Use techniques like k-fold cross-validation to ensure the model generalizes well to unseen data.\n",
    "2. **Regularization**: Apply regularization techniques like L1 (Lasso) or L2 (Ridge) to penalize large coefficients and reduce model complexity.\n",
    "\n",
    "**Underfitting** can be handled by:\n",
    "1. **Increase Model Complexity**: Use a more complex model that can capture the underlying patterns in the data.\n",
    "2. **Feature Engineering**: Add more relevant features or transform existing features to provide the model with more information.\n",
    "\n",
    "### 2d) Explain Gradient Descent in brief.\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the model parameters in the direction of the steepest descent of the cost function until it reaches the minimum. The steps are:\n",
    "1. Initialize the parameters.\n",
    "2. Calculate the gradient of the cost function with respect to the parameters.\n",
    "3. Update the parameters by moving in the direction opposite to the gradient.\n",
    "4. Repeat until convergence.\n",
    "\n",
    "### 2e) What is Lasso Regularization?\n",
    "\n",
    "**Lasso Regularization** (Least Absolute Shrinkage and Selection Operator) is a regularization technique that adds a penalty equal to the absolute value of the magnitude of coefficients to the cost function. It helps in feature selection by shrinking some coefficients to zero, effectively removing them from the model. The Lasso regression objective function is:\n",
    "\n",
    "\\[ \\text{Minimize} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} | \\beta_j | \\right) \\]\n",
    "\n",
    "where \\( \\lambda \\) is the regularization parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokesh\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyforest\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd\\nimport pickle'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pd.read_csv('../data-sets/IPL_IMB_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
